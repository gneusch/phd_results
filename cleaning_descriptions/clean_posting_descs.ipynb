{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import unicodedata\n",
    "import html\n",
    "import textdistance\n",
    "stanza.download('en') # download English model\n",
    "nlp = stanza.Pipeline('en') # initialize English neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Job Posting modell</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobPosting:\n",
    "    def __init__(self, **kwargs):\n",
    "        if 'json' in kwargs:\n",
    "            self.__dict__ = json.loads(kwargs.get(\"json\"))\n",
    "            self.processedDescription = []\n",
    "            self.title_prefixes = []\n",
    "            self.title_suffixes = []\n",
    "            self.title_middle_parts = []\n",
    "            self.title_occupations = []\n",
    "        else:\n",
    "            self.id = kwargs.get(\"_id\")\n",
    "            self.title = kwargs.get(\"title\")\n",
    "            self.date = kwargs.get(\"date\")\n",
    "            self.description = kwargs.get(\"description\")\n",
    "            self.ner_sentences = []\n",
    "            self.ner_tokens = []\n",
    "            self.upos_sentences = []\n",
    "            self.xpos_sentences = []\n",
    "            self.lemmaDescSentences = []\n",
    "            self.originalDescSentences = []\n",
    "            self.processedDescription = []\n",
    "            self.title_prefixes = []\n",
    "            self.title_suffixes = []\n",
    "            self.title_middle_parts = []\n",
    "            self.title_occupations = []\n",
    "        \n",
    "    def __eq__(self, other):\n",
    "        return self.id == other.id\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash(self.id)\n",
    "        \n",
    "    def append_ner_sentences(self, sentences):\n",
    "        self.ner_sentences.append(sentences)\n",
    "\n",
    "    def append_ner_tokens(self, sentences):\n",
    "        self.ner_tokens.append(sentences)\n",
    "\n",
    "    def append_upos_sentences(self, sentences):\n",
    "        self.upos_sentences.append(sentences)\n",
    "\n",
    "    def append_xpos_sentences(self, sentences):\n",
    "        self.xpos_sentences.append(sentences)\n",
    "        \n",
    "    def appendLemmaDescSentence(self, sentence):\n",
    "        self.lemmaDescSentences.append(sentence)\n",
    "        \n",
    "    def appendOriginalDescSentence(self, sentence):\n",
    "        self.originalDescSentences.append(sentence)\n",
    "    \n",
    "    def appendProcessedDescription(self, sentence):\n",
    "        self.processedDescription.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Restore job postings from file</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a loop to avoid loading the hole file into memory\n",
    "import os\n",
    "\n",
    "postings = set()\n",
    "\n",
    "with open(f'cleaning_descriptions/identified_postings_1910.jl') as file_in:    \n",
    "    for line in file_in:\n",
    "        postings.add(JobPosting(json = line))\n",
    "\n",
    "len(postings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Skill modell and data fetching from file</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Skill:\n",
    "    def __init__(self, label):\n",
    "        self.label = label\n",
    "        self.nlpLabel = ''\n",
    "        self.lemmaLabel = ''\n",
    "        \n",
    "    def setNlpLabel(self, nlpLabel):\n",
    "        self.nlpLabel = nlpLabel\n",
    "        \n",
    "    def setLemmaLabel(self, lemmaLabel):\n",
    "        self.lemmaLabel = lemmaLabel\n",
    "\n",
    "skills = set()\n",
    "        \n",
    "with open('cleaning_descriptions/all_ict_skill_labels.csv') as file:\n",
    "    lines = file.readlines()\n",
    "    \n",
    "for skill in [skill.strip() for skill in lines]:\n",
    "    skills.add(Skill(skill))\n",
    "    \n",
    "for skill in skills:\n",
    "    nlpResult = nlp(skill.label)\n",
    "    skill.setNlpLabel(nlpResult)\n",
    "    skill.setLemmaLabel(' '.join([word.lemma for word in nlpResult.sentences[0].words]))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Text preprocessing and similarity calculation</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this and that are list objects\n",
    "#0 <= acceptance <=1 \n",
    "#the higher ts is the more similar thisElem and thatElem\n",
    "def text_similarity(similarityObject, this, that, acceptance):\n",
    "    results = []\n",
    "    for thisElem in this:\n",
    "        for thatElem in that:\n",
    "            ts = similarityObject.similarity(thisElem, thatElem)\n",
    "            nts = similarityObject.normalized_similarity(thisElem, thatElem)\n",
    "            if(nts > acceptance):\n",
    "                results.append((thisElem, thatElem, nts, ts))\n",
    "    return results\n",
    "\n",
    "def text_similarities(similarityObjects, acceptanceBaseObject, this, that, acceptance):\n",
    "    results = []\n",
    "    for thisElem in this:\n",
    "        for thatElem in that:\n",
    "            tss = []\n",
    "            ntss = []\n",
    "            \n",
    "            for i, similarityObject in enumerate(similarityObjects):\n",
    "                nts = similarityObject.normalized_similarity(thisElem, thatElem)\n",
    "                if(i == acceptanceBaseObject and nts <= acceptance):\n",
    "                    break\n",
    "                tss.append(similarityObject.similarity(thisElem, thatElem))\n",
    "                ntss.append(nts)\n",
    "            \n",
    "            if(ntss):\n",
    "                results.append((thisElem, thatElem, ntss, tss))\n",
    "    return results\n",
    "\n",
    "def text_normalized_similarities(similarityObjects, acceptanceBaseObject, this, that, acceptance):\n",
    "    results = []\n",
    "    for thisElem in this:\n",
    "        for thatElem in that:\n",
    "            ntss = []\n",
    "            \n",
    "            for i, similarityObject in enumerate(similarityObjects):\n",
    "                nts = similarityObject.normalized_similarity(thisElem, thatElem)\n",
    "                if(i == acceptanceBaseObject and nts <= acceptance):\n",
    "                    break                \n",
    "                ntss.append(nts)\n",
    "            \n",
    "            if(ntss):\n",
    "                results.append((thisElem, thatElem, ntss))\n",
    "    return results\n",
    "\n",
    "#generate n-grams from an input string where N in [1,...,maxLength]\n",
    "def ngrams(input, maxLength, minLength = 1):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for n in range(minLength, maxLength+1):\n",
    "        for i in range(len(input)-n+1):\n",
    "            output.append(input[i:i+n])\n",
    "    return [' '.join(word) for word in output]\n",
    "\n",
    "def fixed_ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input)-n+1):\n",
    "        output.append(input[i:i+n])\n",
    "    return [' '.join(word) for word in output]\n",
    "\n",
    "def removeMarkupAndControl(text):\n",
    "    text = re.sub('<.*>', ' ', re.sub('</?\\w[^>]*>', ' ', text)) #remove html tags\n",
    "        #text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip() #remove unnecessary control chars and trim\n",
    "    text = ''.join(ch for ch in text if (unicodedata.category(ch)[0]!=\"C\")) #and unicodedata.category(ch)[0]!=\"P\")) #C control, P punctuation http://www.fileformat.info/info/unicode/category/index.htm\n",
    "    text = re.sub(' +', ' ', text) #more than one whitespace    \n",
    "    return text\n",
    "\n",
    "def separateMarkupAndControl(text):\n",
    "    text = re.sub('<.*>', '.', re.sub('</?\\w[^>]*>', '.', text)) #remove html tags\n",
    "        #text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ').strip() #remove unnecessary control chars and trim\n",
    "    text = ''.join(ch for ch in text if (unicodedata.category(ch)[0]!=\"C\")) #and unicodedata.category(ch)[0]!=\"P\")) #C control, P punctuation http://www.fileformat.info/info/unicode/category/index.htm\n",
    "    text = re.sub(' +', ' ', text) #more than one whitespace    \n",
    "    text = re.sub('\\.+', '. ', text)\n",
    "    text = re.sub('^\\. ', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Cleaning descriptions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "identifying_pharses = {\"job description\",\"responsibilities\",\"responsibility\",\"competence\",\"skill\",\"experience\",\"qualification\",\"education\",\"technical requirements\",\n",
    "                       \"the role\",\"core duties\",\"qualities\",\"requirement\",\"ideal candidate\",\"we would like from\",\"personal attributes\",\"activities\",\n",
    "                       \"candidate will have\"}\n",
    "\n",
    "irrelevant_phrases = {\"successful candidate\", \"bank holiday\", \"bank holidays\", \"about us\",\"what we offer\",\"salary is competitive\",\"we would like to give you\",\n",
    "                      \"what can expect from us\",\"equal opportunity\",\"equal opportunities\",\"apply now\",\"bonus scheme\",\"please contact\",\"further information\",\n",
    "                      \"further details\", \"chesterzoo\", \"interviews held\", \"candidate privacy\", \"paste following link\", \"copy paste following\", \"employment agency\", \n",
    "                      \"temporary workers\", \"annual leave\", \"competitive basic salary\", \"competitive salary\", \"criminal conviction\", \"sexual\", \"sexual orientation\", \n",
    "                      \"national origin\", \"disability\", \"ethnicity\", \"diversity\", \"inclusion\", \"gym membership\", \"competitive pay\", \n",
    "                      \"on-site parking\", \"pension scheme\", \"free parking\", \"criminal offence\", \"employee assistance programme\", \"flexible working\", \"Willingness\", \n",
    "                      \"work flexible hours\",\"eligibility to work\", \"flexible approach to working\", \"flexible attitude to working\", \"flexible with working\", \"flexible benefits\",\n",
    "                      \"competative starting salary\",\"development budget\",\"employee assistance\",\"free office fruit\",\"free fruit\",\"days vacation\",\n",
    "                      \"pension contributions\", \"employee support programme\",\"flexi time\", \"flexi-time\",\"cycle to work\",\"salary range\",\"work from home\",\"working from home\",\n",
    "                      \"days holiday\", \"benefits package\", \"employee benefits\", \"0113 \",\"01244\",\"01582\",\"01865\",\"01908\", \"deadline for applications\",\n",
    "                      \"private health care\", \"linkedIn learning\", \"cafe on-site\", \"send cv\", \"send your cv\", \"please send\", \"mon\",\"fri\",\"monday\",\"friday\", \"mondays\",\n",
    "                      \"fridays\", \"weekend\", \"massage\", \"days of the week\", \"days of week\", \"days per week\", \"day's holiday\", \"pension plan\", \"career progression\",\n",
    "                      \"childcare\", \"months free\", \"not eligible\", \"analyst programme\", \"charity\", \"birthday\", \"anniversary\", \"parental\", \"maternity\",\"paternity\",\n",
    "                      \"minutes walk\",\"train station\",\"fantastic team\",\"passionate people\", \"recruitment process\", \"accountability\", \"out of hours\", \"days a week\",\n",
    "                      \"home working\", \"social event\", \"per week\", \"24x7x365\", \"required\"}\n",
    "                      \n",
    "\n",
    "#Patterns that if a paragraph contains, the whole paragraph will be removed. In paragraph I mean <li> tags mostly under a <ul>.\n",
    "irrelevant_phrases_pattern = re.compile(\"|\".join([f'\\\\b{re.escape(ips)}\\\\b' for ips in irrelevant_phrases]), re.I)\n",
    "skill_pattern = re.compile(\"|\".join([f'\\\\b{re.escape(skill.label)}\\\\b' for skill in skills]), re.I)\n",
    "web_pattern = re.compile(\"www\\\\ [a-zA-Z0-9]*\\\\ com|www\\\\.[a-zA-Z0-9]*\\\\.com|http://|https://\")\n",
    "pound_pattern = re.compile(\"£[0-9]*\")\n",
    "hour_pattern = re.compile(\"\\\\b(\\d{1,2})(am|pm)\\\\b\")\n",
    "month_pattern = re.compile(\"\\\\b(\\d{1,2})(\\s{0,1})months\\\\b\")\n",
    "\n",
    "corp_spec_stopwords = {\"year\", \"years\", \"preferred\", \"equivalent\", \"experience\", \"proficient\", \"like\", \"000\", \"ability\", \"able\", \"etc\", \"field\", \"minimum\", \n",
    "                       \"must\", \"excellent\", \"outstanding\", \"strong\", \"would\", \"advantageous\", \"beneficial\", \"mandatory\", \"within\", \"24X7\"}\n",
    "#.^$*+?()[{\\|\n",
    "punct_list = r'[“”‘’!\"\\$%&\\'\\(\\)\\*+,\\./:;<=>?@\\[\\]\\^`\\{\\|\\}~\\\\]' #-–_\n",
    "punct_map = punct = {'’':' ','!':' ','\"':' ','$':' ','%':' ','&':' ','\\'':' ','(':' ',')':' ','*':' ','+':' ',',':' ','-':' ','.':' ','/':' ',':':' ',';':' ','<':' ','=':' ','>':' ','?':' ','@':' ','[':' ','\\\\':' ',']':' ','^':' ','_':' ','`':' ','{':' ','|':' ','}':' ','~':' ','–':' '}\n",
    "change_dict = {'&amp;':'&'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Approach 1: decompose job descriptions and retain informative parts</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5231\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def find_list_sibling(t):\n",
    "    next_sibling = t.next_sibling\n",
    "    for i in range(3):\n",
    "        if next_sibling:\n",
    "            if next_sibling.name == 'ul':\n",
    "                return next_sibling\n",
    "            else:\n",
    "                next_sibling = next_sibling.next_sibling\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "for posting in postings:\n",
    "        soup = BeautifulSoup(' '.join(posting.description).replace('\\n', '').replace('\\r', ''), \"html.parser\")\n",
    "        all_ps = soup.find_all('p')\n",
    "        all_hs = soup.find_all(re.compile('^h[1-6]$'))\n",
    "        informative_list = set()\n",
    "        for t in all_ps + all_hs:\n",
    "            for ip in identifying_pharses:\n",
    "                if ip in t.get_text().lower():\n",
    "                    ul = find_list_sibling(t)\n",
    "                    if ul:\n",
    "                        informative_list.add(ul)\n",
    "                    else:\n",
    "                        if t.next_sibling and isinstance(t.next_sibling, bs4.element.Tag):\n",
    "                            if 'ul' in [c.name for c in t.next_sibling.contents]:\n",
    "                                for c in t.next_sibling.contents:\n",
    "                                    if c.name == 'ul':\n",
    "                                        informative_list.add(c)\n",
    "                    if not ul:\n",
    "                        if t.parent.next_sibling and isinstance(t.parent.next_sibling, bs4.element.Tag):\n",
    "                            if 'ul' in [c.name for c in t.parent.next_sibling.contents]:\n",
    "                                for c in t.parent.next_sibling.contents:\n",
    "                                    if c.name == 'ul':\n",
    "                                        informative_list.add(c)\n",
    "        if not informative_list:\n",
    "            #display(HTML(' '.join(posting.description)))\n",
    "            all_uls = soup.find_all('ul')\n",
    "            for ul in all_uls:\n",
    "                skill_match = re.findall(skill_pattern, ul.get_text(\" \", strip=True))\n",
    "                irrelevant_phrases_match = re.findall(irrelevant_phrases_pattern, ul.get_text(\" \", strip=True))\n",
    "                if skill_match and not irrelevant_phrases_match:\n",
    "                    informative_list.add(ul)\n",
    "                    \n",
    "        if not informative_list:\n",
    "            for t in soup.findAll(text=True):\n",
    "                skill_match = re.findall(skill_pattern, t)\n",
    "                irrelevant_phrases_match = re.findall(irrelevant_phrases_pattern, t)\n",
    "                if skill_match and not irrelevant_phrases_match:\n",
    "                    informative_list.add(t.strip())\n",
    "                    \n",
    "        if informative_list:\n",
    "             posting.processedDescription = list(informative_list)\n",
    "                \n",
    "print(len([p for p in postings if p.processedDescription]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for posting in postings:\n",
    "    posting.originalDescSentences = posting.processedDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for posting in postings:\n",
    "    posting.processedDescription = posting.originalDescSentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>remove li or paragraph that contain one of the irrelevant patterns</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(postings):\n",
    "    for paragraph in p.processedDescription:\n",
    "        if isinstance(paragraph, bs4.element.Tag):\n",
    "            if paragraph.name == 'ul':\n",
    "                for li in paragraph.find_all('li'):\n",
    "                    if re.findall(irrelevant_phrases_pattern, str(li)) or re.findall(web_pattern, str(li)) or re.findall(pound_pattern, str(li)) or re.findall(hour_pattern, str(li)) or re.findall(month_pattern, str(li)):\n",
    "                        li.decompose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(postings):\n",
    "    not_irrelevant = []\n",
    "    for d in p.processedDescription:\n",
    "        if not (re.findall(irrelevant_phrases_pattern, str(d)) or re.findall(web_pattern, str(d)) or re.findall(pound_pattern, str(d)) or re.findall(hour_pattern, str(d)) or re.findall(month_pattern, str(d))):\n",
    "            not_irrelevant.append(str(d))\n",
    "    p.processedDescription = not_irrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>remove markup, control, punctuation, corpus specific stopwords, and tokenize</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(postings):\n",
    "    pd_texts = []\n",
    "    for pd in p.processedDescription:\n",
    "        #text = f'{pd.text} ' if isinstance(pd, bs4.element.Tag) else pd\n",
    "        #pd_texts.append(removeMarkupAndControl(pd).strip())\n",
    "        pd_texts.append(separateMarkupAndControl(pd).strip())\n",
    "    p.processedDescription = pd_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, posting in enumerate(postings):\n",
    "    texts = []\n",
    "    for paragraph in posting.processedDescription:\n",
    "        for sentence in tokenizer.tokenize(paragraph): #create sentences\n",
    "            sentence = re.sub('&amp;', '&', re.sub('\\d{1,3}\\)', '', re.sub('(^-|–|_\\s)|(\\s-|–|_\\s)', '', re.sub('0ffice', 'Office', re.sub('0365', 'O365', sentence)))))\n",
    "            #sentence = sentence.translate(str.maketrans(punct)).lower() #remove punctuations\n",
    "            sentence = ' '.join([w for w in word_tokenize(sentence, preserve_line=True) if w.lower() not in stop_words.union(corp_spec_stopwords) or w == 'IT']) #remove sw\n",
    "            for s in re.split(punct_list, sentence):\n",
    "                if s.strip() and len(s.strip()) > 2:\n",
    "                    texts.append(s.strip())\n",
    "    seen = set()    \n",
    "    posting.processedDescription = [x for x in texts if not (x in seen or seen.add(x))]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, posting in enumerate(postings):\n",
    "    sentences = []\n",
    "    for sentence in posting.processedDescription:\n",
    "        if len(sentence.split(' ')) > 3:\n",
    "            sentences = sentences + ngrams(sentence, 4, 2)\n",
    "        else:\n",
    "            sentences.append(sentence)    \n",
    "    \n",
    "    posting.processedDescription = [d for d in sentences if not (re.findall(irrelevant_phrases_pattern, str(d)) or re.findall(web_pattern, str(d)) or re.findall(pound_pattern, str(d)) or re.findall(hour_pattern, str(d)) or re.findall(month_pattern, str(d)))]\n",
    "    #for s in sentences:\n",
    "    #    if len(s.split(' ')) < 3:\n",
    "    #        nlp_result = nlp(s)\n",
    "    #        upos = ' '.join([word.upos for word in nlp_result.sentences[0].words])\n",
    "    #        print(f'{s} - {upos}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_start_pattern = re.compile(\"^\\d+\\\\b\")\n",
    "for p in postings:\n",
    "    new_p_desc = []\n",
    "    for li in p.processedDescription:\n",
    "        if re.findall(num_start_pattern, li): \n",
    "            number = re.findall(num_start_pattern, li)\n",
    "            if not new_p_desc:\n",
    "                if len(li.split(' ')) > 2:\n",
    "                    new_p_desc.append(' '.join(li.split(\" \")[1:]))\n",
    "                else:\n",
    "                    continue\n",
    "            elif re.findall(re.compile(\"^\\d+$\"), li):\n",
    "                new_p_desc[-1] = f'{new_p_desc[-1]} {li}'\n",
    "            elif new_p_desc[-1].split(' ')[-1] == number[0]:\n",
    "                new_p_desc[-1] = f'{\" \".join(new_p_desc[-1].split(\" \")[:-1])} {li}'\n",
    "            elif number[0] in new_p_desc[-1]:\n",
    "                new_p_desc[-1] = f'{\" \".join([new_p_desc[-1]] + [w for w in li.split(\" \") if w not in new_p_desc[-1]])}'\n",
    "            else:\n",
    "                if len(number[0]) < 4 and len(li.split(' ')) < 3:\n",
    "                    continue\n",
    "                elif len(number[0]) < 4:\n",
    "                    new_p_desc.append(' '.join(li.split(\" \")[1:]))\n",
    "                else:\n",
    "                    new_p_desc[-1] = f'{new_p_desc[-1]} {li}'\n",
    "        else:\n",
    "            new_p_desc.append(li)\n",
    "        p.processedDescription = new_p_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in postings:\n",
    "    new_p_desc = []\n",
    "    for li in p.processedDescription:\n",
    "        if re.findall(re.compile(\"^# #\"), li): \n",
    "            if len(li.split(' ')) > 2:\n",
    "                new_p_desc.append(' '.join(li.split(\" \")[2:]))\n",
    "            else:\n",
    "                continue\n",
    "        elif re.findall(re.compile(\"^#\"), li):\n",
    "            new_p_desc.append(' '.join(li.split(\" \")[1:]))\n",
    "        else:\n",
    "            new_p_desc.append(li)\n",
    "        p.processedDescription = new_p_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Test and copy</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5018\n"
     ]
    }
   ],
   "source": [
    "rawPostings = [p for p in postings if len(p.processedDescription) > 2]\n",
    "print(len(rawPostings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pattern = re.compile(\"^#\")\n",
    "for i, p in enumerate(rawPostings):\n",
    "    for j, li in enumerate(p.processedDescription):\n",
    "        if re.findall(re.compile(\"^\\d+\\\\b\"), str(li)) or re.findall(irrelevant_phrases_pattern, str(li)) or re.findall(web_pattern, str(li)) or re.findall(pound_pattern, str(li)) or re.findall(hour_pattern, str(li)) or re.findall(temp_pattern, str(li)):\n",
    "            print(f'---{li}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
